{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/multimediateam/miniconda3/envs/depth/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Resnet_depth import *\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Depth(nn.Module):\n",
    "    def __init__(self, decoder, output_size, in_channels=3, pretrained=True) -> None:\n",
    "        super(Depth,self).__init__()\n",
    "        self.model_s=timm.create_model('lcnet_050.ra2_in1k', pretrained=True)\n",
    "        selected_layers = list(self.model_s.children())[:4]\n",
    "\n",
    "        self.feat = torch.nn.Sequential(*selected_layers)\n",
    "        \n",
    "\n",
    "        \n",
    "        num_channels=512\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.conv2 = nn.Conv2d(num_channels,num_channels//2,kernel_size=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels//2)\n",
    "        self.decoder = choose_decoder(decoder, num_channels//2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(num_channels//32,1,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bilinear = nn.Upsample(size=self.output_size, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv2.apply(weights_init)\n",
    "        self.bn2.apply(weights_init)\n",
    "        self.decoder.apply(weights_init)\n",
    "        self.conv3.apply(weights_init)\n",
    "    def forward(self,x):\n",
    "        x=self.feat(x)\n",
    "        \n",
    "        x=self.conv2(x)\n",
    "        x=self.bn2(x)\n",
    "        \n",
    "        # decoder\n",
    "        x = self.decoder(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bilinear(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Resnet_depth import *\n",
    "from Depth_net import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s=timm.create_model('lcnet_050.ra2_in1k', pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 256, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data=np.load(\"/home/multimediateam/Documents/Vision_HUST/DepthEstimation/CitySpaces/data/train/depth/0.npy\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "\n",
    "def measure_latency_cpu_usage(model, test_inputs):\n",
    "    process = psutil.Process()\n",
    "    cpu_start = process.cpu_percent()\n",
    "    start = time.time()\n",
    "    model.eval().cuda()\n",
    "    predictions = model(test_inputs)\n",
    "    end = time.time()\n",
    "    cpu_end = process.cpu_percent()\n",
    "    latency = end - start\n",
    "    cpu_usage = cpu_end - cpu_start\n",
    "    return latency, cpu_usage\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "def measure_gpu_throughput(model, inputs, batch_size):\n",
    "    inputs = inputs.to('cuda')\n",
    "    model = model.to('cuda')\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    predictions=model(inputs)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    latency = start.elapsed_time(end)\n",
    "    throughput = inputs.size(0) / latency\n",
    "    return throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/multimediateam/miniconda3/envs/depth/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/multimediateam/miniconda3/envs/depth/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 512, 1, 1], expected input[1, 8, 114, 152] to have 512 channels, but got 8 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_pplc\u001b[39m=\u001b[39mDepth(decoder\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mupconv\u001b[39m\u001b[39m\"\u001b[39m,output_size\u001b[39m=\u001b[39m(\u001b[39m228\u001b[39m,\u001b[39m304\u001b[39m),pretrained\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_resnet\u001b[39m=\u001b[39mResNet(layers\u001b[39m=\u001b[39m\u001b[39m18\u001b[39m,decoder\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mupconv\u001b[39m\u001b[39m\"\u001b[39m,output_size\u001b[39m=\u001b[39m(\u001b[39m228\u001b[39m,\u001b[39m304\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(measure_latency_cpu_usage(model_pplc,torch\u001b[39m.\u001b[39;49mrand(\u001b[39m1\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m228\u001b[39;49m,\u001b[39m304\u001b[39;49m)\u001b[39m.\u001b[39;49mcuda()))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGPU:\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(measure_gpu_throughput(model_pplc,inputs\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m228\u001b[39m,\u001b[39m304\u001b[39m),batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(measure_latency_cpu_usage(model_resnet,torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m228\u001b[39m,\u001b[39m304\u001b[39m)\u001b[39m.\u001b[39mcuda()))\n",
      "\u001b[1;32m/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39meval()\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m predictions \u001b[39m=\u001b[39m model(test_inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m cpu_end \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mcpu_percent()\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Vision_HUST/DepthEstimation/Depth_net.py:30\u001b[0m, in \u001b[0;36mDepth.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m     28\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeat(x)\n\u001b[0;32m---> 30\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x)\n\u001b[1;32m     31\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(x)\n\u001b[1;32m     33\u001b[0m     \u001b[39m# decoder\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 512, 1, 1], expected input[1, 8, 114, 152] to have 512 channels, but got 8 channels instead"
     ]
    }
   ],
   "source": [
    "model_pplc=Depth(decoder=\"upconv\",output_size=(228,304),pretrained=False)\n",
    "model_resnet=ResNet(layers=18,decoder=\"upconv\",output_size=(228,304))\n",
    "\n",
    "\n",
    "print(measure_latency_cpu_usage(model_pplc,torch.rand(1,3,228,304).cuda()))\n",
    "print(\"GPU:{}\".format(measure_gpu_throughput(model_pplc,inputs=torch.rand(1,3,228,304),batch_size=1)))\n",
    "\n",
    "print(measure_latency_cpu_usage(model_resnet,torch.rand(1,3,228,304).cuda()))\n",
    "print(\"GPU:{}\".format(measure_gpu_throughput(model_resnet,inputs=torch.rand(1,3,228,304),batch_size=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 512, 1, 1], expected input[32, 8, 114, 152] to have 512 channels, but got 8 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m input_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m32\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m228\u001b[39m, \u001b[39m304\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Sử dụng torch.utils.benchmark để đo đạc thời gian inference\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m benchmark_result \u001b[39m=\u001b[39m benchmark\u001b[39m.\u001b[39;49mTimer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     stmt\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmodel(input_data)\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mglobals\u001b[39;49m\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m: model_pplc\u001b[39m.\u001b[39;49mcuda(), \u001b[39m'\u001b[39;49m\u001b[39minput_data\u001b[39;49m\u001b[39m'\u001b[39;49m: input_data\u001b[39m.\u001b[39;49mcuda()}\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\u001b[39m.\u001b[39;49mtimeit(\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m throughput \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m \u001b[39m/\u001b[39m benchmark_result\u001b[39m.\u001b[39mmedian\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThroughput: \u001b[39m\u001b[39m{\u001b[39;00mthroughput\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m images/s\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/utils/benchmark/utils/timer.py:266\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Mirrors the semantics of timeit.Timer.timeit().\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \n\u001b[1;32m    261\u001b[0m \u001b[39mExecute the main statement (`stmt`) `number` times.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39mhttps://docs.python.org/3/library/timeit.html#timeit.Timer.timeit\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39mwith\u001b[39;00m common\u001b[39m.\u001b[39mset_torch_threads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_spec\u001b[39m.\u001b[39mnum_threads):\n\u001b[1;32m    265\u001b[0m     \u001b[39m# Warmup\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeit(number\u001b[39m=\u001b[39;49m\u001b[39mmax\u001b[39;49m(\u001b[39mint\u001b[39;49m(number \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m100\u001b[39;49m), \u001b[39m2\u001b[39;49m))\n\u001b[1;32m    268\u001b[0m     \u001b[39mreturn\u001b[39;00m common\u001b[39m.\u001b[39mMeasurement(\n\u001b[1;32m    269\u001b[0m         number_per_run\u001b[39m=\u001b[39mnumber,\n\u001b[1;32m    270\u001b[0m         raw_times\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeit(number\u001b[39m=\u001b[39mnumber)],\n\u001b[1;32m    271\u001b[0m         task_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_spec\n\u001b[1;32m    272\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/utils/benchmark/utils/timer.py:256\u001b[0m, in \u001b[0;36mTimer._timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_timeit\u001b[39m(\u001b[39mself\u001b[39m, number: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m    254\u001b[0m     \u001b[39m# Even calling a timer in C++ takes ~50 ns, so no real operation should\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[39m# take less than 1 ns. (And this prevents divide by zero errors.)\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timer\u001b[39m.\u001b[39;49mtimeit(number), \u001b[39m1e-9\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/timeit.py:177\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    175\u001b[0m gc\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m    176\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     timing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(it, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimer)\n\u001b[1;32m    178\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<timeit-src>:6\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Vision_HUST/DepthEstimation/Depth_net.py:30\u001b[0m, in \u001b[0;36mDepth.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m     28\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeat(x)\n\u001b[0;32m---> 30\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x)\n\u001b[1;32m     31\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(x)\n\u001b[1;32m     33\u001b[0m     \u001b[39m# decoder\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 512, 1, 1], expected input[32, 8, 114, 152] to have 512 channels, but got 8 channels instead"
     ]
    }
   ],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "input_data = torch.randn(32, 3, 228, 304)\n",
    "\n",
    "# Sử dụng torch.utils.benchmark để đo đạc thời gian inference\n",
    "benchmark_result = benchmark.Timer(\n",
    "    stmt='model(input_data)',\n",
    "    globals={'model': model_pplc.cuda(), 'input_data': input_data.cuda()}\n",
    ").timeit(100)\n",
    "throughput = 100 / benchmark_result.median\n",
    "\n",
    "print(f'Throughput: {throughput:.2f} images/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/multimediateam/miniconda3/envs/depth/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/multimediateam/miniconda3/envs/depth/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from Resnet_depth import *\n",
    "from Depth_net import Depth\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision.transforms import transforms as t\n",
    "from PIL import Image\n",
    "model_resnet=ResNet(layers=18,decoder=\"upconv\",output_size=(228,304))\n",
    "model_pplc=Depth(decoder=\"upconv\",output_size=(228,912))\n",
    "valid_tfms = t.Compose([\n",
    "    t.ToTensor(),\n",
    "    t.Resize((228,912)),\n",
    "    t.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    t.ToPILImage()\n",
    "    \n",
    "    \n",
    "])\n",
    "ROOT=\"KITTI_test\"\n",
    "list_dir=os.listdir(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (Image, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Image!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Image!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m img\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mcvtColor(img,cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mvalid_tfms(img)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m output\u001b[39m=\u001b[39mmodel_pplc(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/multimediateam/Documents/Vision_HUST/DepthEstimation/test.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Vision_HUST/DepthEstimation/Depth_net.py:28\u001b[0m, in \u001b[0;36mDepth.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[0;32m---> 28\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeat(x)\n\u001b[1;32m     30\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n\u001b[1;32m     31\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/depth/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (Image, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Image!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!Image!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "model_pplc.load_state_dict(torch.load('result/base_line_KITTI_PPLC-net/best_model.pth'),strict=False)\n",
    "for filename in list_dir:\n",
    "    file_image_path=os.path.join(ROOT,filename)\n",
    "    img=cv2.imread(file_image_path)\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    input=valid_tfms(img)\n",
    "    \n",
    "    output=model_pplc(input)\n",
    "    print(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 228, 304])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=cv2.imread(\"nyu_data/data/nyu2_test/00001_colors.png\")\n",
    "H,W=img.shape[:2]\n",
    "img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "img=cv2.resize(img,(304,228))\n",
    "norm_transform= t.Compose([\n",
    "    t.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),    \n",
    "])\n",
    "img=torch.from_numpy(img).permute(2,0,1).unsqueeze(0).cuda().float()\n",
    "img=norm_transform(img)\n",
    "print(img.shape)\n",
    "model_resnet.load_state_dict(torch.load('result/base_line_NYU_V2_Resnet/best_model.pth'),strict=False)\n",
    "model_resnet.cuda().eval()\n",
    "\n",
    "output=model_resnet(img)\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "                output,\n",
    "                size=(H,W),\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            ).squeeze()\n",
    "depth_map = output.detach().cpu().numpy()\n",
    "depth_map=np.squeeze(depth_map)\n",
    "depth_map = (depth_map*25.5).astype(np.uint8)\n",
    "depth_map = cv2.applyColorMap(depth_map , cv2.COLORMAP_MAGMA)\n",
    "cv2.imwrite(\"results.png\",depth_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(t.Normalize):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        mean=(0.485, 0.456, 0.406)\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "        new_mean = [-m/s for m,s in zip(mean,std)]\n",
    "        new_std = [1/s for s in std]\n",
    "        super().__init__(new_mean, new_std, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def colored_depthmap(depth, d_min=None, d_max=None,cmap=plt.cm.inferno):\n",
    "    if d_min is None:\n",
    "        d_min = np.min(depth)\n",
    "    if d_max is None:\n",
    "        d_max = np.max(depth)\n",
    "    depth_relative = (depth - d_min) / (d_max - d_min)\n",
    "    return 255 * cmap(depth_relative)[:,:,:3] # H, W, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 228, 304])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=cv2.imread(\"nyu_data/data/nyu2_test/00001_colors.png\")\n",
    "H,W=img.shape[:2]\n",
    "img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "img=cv2.resize(img,(304,228))\n",
    "norm_transform= t.Compose([\n",
    "    t.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),    \n",
    "])\n",
    "img=torch.from_numpy(img).permute(2,0,1).unsqueeze(0).cuda().float()\n",
    "img=norm_transform(img)\n",
    "print(img.shape)\n",
    "model_resnet.load_state_dict(torch.load('result/base_line_NYU_V2_Resnet/best_model.pth'),strict=False)\n",
    "model_resnet.cuda().eval()\n",
    "\n",
    "output=model_resnet(img)\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "                output,\n",
    "                size=(H,W),\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            ).squeeze()\n",
    "depth_map = output.detach().cpu().numpy()\n",
    "depth_map=np.squeeze(depth_map)\n",
    "depth_map=colored_depthmap(depth_map , cv2.COLORMAP_MAGMA)\n",
    "cv2.imwrite(\"results.png\",depth_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  12.4 M  \n",
      "fwd MACs:                                                               3.7104 GMACs\n",
      "fwd FLOPs:                                                              7.434 GFLOPS\n",
      "fwd+bwd MACs:                                                           11.1311 GMACs\n",
      "fwd+bwd FLOPs:                                                          22.302 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "ResNet(\n",
      "  12.4 M = 100% Params, 3.71 GMACs = 100% MACs, 7.43 GFLOPS = 49.9106% FLOPs\n",
      "  (conv1): Conv2d(9.41 K = 0.0759% Params, 163.02 MMACs = 4.3937% MACs, 326.04 MFLOPS = 2.1929% FLOPs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(128 = 0.001% Params, 0 MACs = 0% MACs, 2.22 MFLOPS = 0% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.11 MFLOPS = 0% FLOPs, inplace=True)\n",
      "  (maxpool): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 1.11 MFLOPS = 0% FLOPs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    147.97 K = 1.1936% Params, 638.78 MMACs = 17.2161% MACs, 1.28 GFLOPS = 8.5927% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      73.98 K = 0.5968% Params, 319.39 MMACs = 8.6081% MACs, 640.44 MFLOPS = 4.2963% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 0.2974% Params, 159.69 MMACs = 4.304% MACs, 319.39 MFLOPS = 2.1482% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.001% Params, 0 MACs = 0% MACs, 554.5 KFLOPS = 0% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 554.5 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.2974% Params, 159.69 MMACs = 4.304% MACs, 319.39 MFLOPS = 2.1482% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.001% Params, 0 MACs = 0% MACs, 554.5 KFLOPS = 0% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      73.98 K = 0.5968% Params, 319.39 MMACs = 8.6081% MACs, 640.44 MFLOPS = 4.2963% FLOPs\n",
      "      (conv1): Conv2d(36.86 K = 0.2974% Params, 159.69 MMACs = 4.304% MACs, 319.39 MFLOPS = 2.1482% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128 = 0.001% Params, 0 MACs = 0% MACs, 554.5 KFLOPS = 0% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 554.5 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(36.86 K = 0.2974% Params, 159.69 MMACs = 4.304% MACs, 319.39 MFLOPS = 2.1482% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128 = 0.001% Params, 0 MACs = 0% MACs, 554.5 KFLOPS = 0% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    525.57 K = 4.2396% Params, 577.77 MMACs = 15.5717% MACs, 1.16 GFLOPS = 7.7719% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      230.14 K = 1.8565% Params, 252.77 MMACs = 6.8126% MACs, 506.67 MFLOPS = 3.4002% FLOPs\n",
      "      (conv1): Conv2d(73.73 K = 0.5947% Params, 81.25 MMACs = 2.1898% MACs, 162.5 MFLOPS = 1.0929% FLOPs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0.0021% Params, 0 MACs = 0% MACs, 282.11 KFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 282.11 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(147.46 K = 1.1895% Params, 162.5 MMACs = 4.3795% MACs, 324.99 MFLOPS = 2.1859% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0.0021% Params, 0 MACs = 0% MACs, 282.11 KFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        8.45 K = 0.0681% Params, 9.03 MMACs = 0.2433% MACs, 18.34 MFLOPS = 0.1214% FLOPs\n",
      "        (0): Conv2d(8.19 K = 0.0661% Params, 9.03 MMACs = 0.2433% MACs, 18.06 MFLOPS = 0.1214% FLOPs, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256 = 0.0021% Params, 0 MACs = 0% MACs, 282.11 KFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      295.42 K = 2.3831% Params, 324.99 MMACs = 8.7591% MACs, 650.83 MFLOPS = 4.3717% FLOPs\n",
      "      (conv1): Conv2d(147.46 K = 1.1895% Params, 162.5 MMACs = 4.3795% MACs, 324.99 MFLOPS = 2.1859% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256 = 0.0021% Params, 0 MACs = 0% MACs, 282.11 KFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 282.11 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(147.46 K = 1.1895% Params, 162.5 MMACs = 4.3795% MACs, 324.99 MFLOPS = 2.1859% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256 = 0.0021% Params, 0 MACs = 0% MACs, 282.11 KFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    2.1 M = 16.9376% Params, 597.69 MMACs = 16.1087% MACs, 1.2 GFLOPS = 8.0399% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      919.04 K = 7.4136% Params, 261.49 MMACs = 7.0475% MACs, 523.56 MFLOPS = 3.5175% FLOPs\n",
      "      (conv1): Conv2d(294.91 K = 2.379% Params, 84.05 MMACs = 2.2653% MACs, 168.1 MFLOPS = 1.1306% FLOPs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0.0041% Params, 0 MACs = 0% MACs, 145.92 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 145.92 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(589.82 K = 4.7579% Params, 168.1 MMACs = 4.5306% MACs, 336.2 MFLOPS = 2.2612% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0.0041% Params, 0 MACs = 0% MACs, 145.92 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        33.28 K = 0.2685% Params, 9.34 MMACs = 0.2517% MACs, 18.82 MFLOPS = 0.1256% FLOPs\n",
      "        (0): Conv2d(32.77 K = 0.2643% Params, 9.34 MMACs = 0.2517% MACs, 18.68 MFLOPS = 0.1256% FLOPs, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512 = 0.0041% Params, 0 MACs = 0% MACs, 145.92 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      1.18 M = 9.5241% Params, 336.2 MMACs = 9.0611% MACs, 672.84 MFLOPS = 4.5225% FLOPs\n",
      "      (conv1): Conv2d(589.82 K = 4.7579% Params, 168.1 MMACs = 4.5306% MACs, 336.2 MFLOPS = 2.2612% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512 = 0.0041% Params, 0 MACs = 0% MACs, 145.92 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 145.92 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(589.82 K = 4.7579% Params, 168.1 MMACs = 4.5306% MACs, 336.2 MFLOPS = 2.2612% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512 = 0.0041% Params, 0 MACs = 0% MACs, 145.92 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    8.39 M = 67.7093% Params, 671.09 MMACs = 18.0869% MACs, 1.34 GFLOPS = 9.0273% FLOPs\n",
      "    (0): BasicBlock(\n",
      "      3.67 M = 29.6295% Params, 293.6 MMACs = 7.913% MACs, 587.53 MFLOPS = 3.9494% FLOPs\n",
      "      (conv1): Conv2d(1.18 M = 9.5158% Params, 94.37 MMACs = 2.5435% MACs, 188.74 MFLOPS = 1.2695% FLOPs, 256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0.0083% Params, 0 MACs = 0% MACs, 81.92 KFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 81.92 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M = 19.0316% Params, 188.74 MMACs = 5.0869% MACs, 377.49 MFLOPS = 2.5389% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0.0083% Params, 0 MACs = 0% MACs, 81.92 KFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        132.1 K = 1.0656% Params, 10.49 MMACs = 0.2826% MACs, 21.05 MFLOPS = 0.1411% FLOPs\n",
      "        (0): Conv2d(131.07 K = 1.0573% Params, 10.49 MMACs = 0.2826% MACs, 20.97 MFLOPS = 0.1411% FLOPs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1.02 K = 0.0083% Params, 0 MACs = 0% MACs, 81.92 KFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      4.72 M = 38.0798% Params, 377.49 MMACs = 10.1739% MACs, 755.22 MFLOPS = 5.0779% FLOPs\n",
      "      (conv1): Conv2d(2.36 M = 19.0316% Params, 188.74 MMACs = 5.0869% MACs, 377.49 MFLOPS = 2.5389% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 K = 0.0083% Params, 0 MACs = 0% MACs, 81.92 KFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 81.92 KFLOPS = 0% FLOPs, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M = 19.0316% Params, 188.74 MMACs = 5.0869% MACs, 377.49 MFLOPS = 2.5389% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 K = 0.0083% Params, 0 MACs = 0% MACs, 81.92 KFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv2): Conv2d(131.07 K = 1.0573% Params, 10.49 MMACs = 0.2826% MACs, 20.97 MFLOPS = 0.1411% FLOPs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(512 = 0.0041% Params, 0 MACs = 0% MACs, 40.96 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (decoder): UpConv(\n",
      "    1.09 M = 8.7804% Params, 1.05 GMACs = 28.2608% MACs, 2.1 GFLOPS = 14.1051% FLOPs\n",
      "    (layer1): Sequential(\n",
      "      819.46 K = 6.6103% Params, 262.14 MMACs = 7.0652% MACs, 524.41 MFLOPS = 3.5263% FLOPs\n",
      "      (unpool): Unpool(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "      (conv): Conv2d(819.2 K = 6.6082% Params, 262.14 MMACs = 7.0652% MACs, 524.29 MFLOPS = 3.5263% FLOPs, 256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (batchnorm): BatchNorm2d(256 = 0.0021% Params, 0 MACs = 0% MACs, 81.92 KFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 40.96 KFLOPS = 0% FLOPs)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      204.93 K = 1.6531% Params, 262.14 MMACs = 7.0652% MACs, 524.53 MFLOPS = 3.5263% FLOPs\n",
      "      (unpool): Unpool(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "      (conv): Conv2d(204.8 K = 1.652% Params, 262.14 MMACs = 7.0652% MACs, 524.29 MFLOPS = 3.5263% FLOPs, 128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (batchnorm): BatchNorm2d(128 = 0.001% Params, 0 MACs = 0% MACs, 163.84 KFLOPS = 0% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 81.92 KFLOPS = 0% FLOPs)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      51.26 K = 0.4135% Params, 262.14 MMACs = 7.0652% MACs, 524.78 MFLOPS = 3.5263% FLOPs\n",
      "      (unpool): Unpool(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "      (conv): Conv2d(51.2 K = 0.413% Params, 262.14 MMACs = 7.0652% MACs, 524.29 MFLOPS = 3.5263% FLOPs, 64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (batchnorm): BatchNorm2d(64 = 0.0005% Params, 0 MACs = 0% MACs, 327.68 KFLOPS = 0% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 163.84 KFLOPS = 0% FLOPs)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      12.83 K = 0.1035% Params, 262.14 MMACs = 7.0652% MACs, 525.27 MFLOPS = 3.5263% FLOPs\n",
      "      (unpool): Unpool(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "      (conv): Conv2d(12.8 K = 0.1033% Params, 262.14 MMACs = 7.0652% MACs, 524.29 MFLOPS = 3.5263% FLOPs, 32, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (batchnorm): BatchNorm2d(32 = 0.0003% Params, 0 MACs = 0% MACs, 655.36 KFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 327.68 KFLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(144 = 0.0012% Params, 2.95 MMACs = 0.0795% MACs, 5.9 MFLOPS = 0.0397% FLOPs, 16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bilinear): Upsample(0 = 0% Params, 0 MACs = 0% MACs, 69.31 KFLOPS = 0% FLOPs, size=(228, 304), mode=bilinear)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PPLC_net FLOPs:7.434 GFLOPS   MACs:3.7104 GMACs   Params:12.3967 M \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/multimediateam/miniconda3/envs/depth/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/multimediateam/miniconda3/envs/depth/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from Depth_net import Depth\n",
    "from Resnet_depth import ResNet\n",
    "from calflops import calculate_flops\n",
    "model_pplc=Depth(decoder=\"upconv\",output_size=(228,304),pretrained=False)\n",
    "model_resnet=ResNet(layers=18,decoder=\"upconv\",output_size=(228,304))\n",
    "batch_size=1\n",
    "input_shape = (batch_size, 3, 228, 304)\n",
    "flops, macs, params = calculate_flops(model=model_resnet, \n",
    "                                      input_shape=input_shape,\n",
    "                                      output_as_string=True,\n",
    "                                      output_precision=4)\n",
    "print(\"PPLC_net FLOPs:%s   MACs:%s   Params:%s \\n\" %(flops, macs, params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
